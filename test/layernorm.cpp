#include <gtest/gtest.h>
#include "layernorm.h"
#include "attention.h"
#include "matmul.h"

// Layer normalization, forward call.
TEST(LayerNormTest, forward_call) {
    // (1, 10, 2)
    float x[20] = {0.0, 1.1, -0.2, 3.3, 2.4, 1.5, 0.6, 2.7, 0.8, 0.65,
                   2.12, 6.78, 2.2, 9.1, 3.4, 1.55, -1.6, -2.7, -0.08, -0.65};

    // (1, 10)
    float mean[10] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
    // (1, 10)
    float rstd[10] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};

    // (2)
    float weight[2] = {0.5, 0.3};
    // (2)
    float bias[2] = {0.02, -0.01};

    // (1, 10, 2)
    float x_normalized[20] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                              0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};

    // (2, 6)
    float w[12] = {0.12, -1.2, -0.7, 1.0, -1.3, 1.45,
                   0.08, 0.36, 1.6, -0.34, 0.87, 0.25};

    // (6)
    float b[6] = {0.02, 0.03, 0.05, -0.02, -0.03, -0.01};

    // (1, 10, 6)
    float qkv_proj[60] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
    // (1, 2, 10, 10)
    float preattn[200] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                          0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};

    // (1, 2, 10, 10)
    float attn[200] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};

    // (1, 10, 2)
    float y[20] = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
    // conv_y_truth is pytorch tensor output.
    /* Python code

    torch.set_printoptions(precision=6)
    x = [0.0, 1.1, -0.2, 3.3, 2.4, 1.5, 0.6, 2.7, 0.8, 0.65,
         2.12, 6.78, 2.2, 9.1, 3.4, 1.55, -1.6, -2.7, -0.08, -0.65]
    x = torch.tensor(x, dtype=torch.float32).view(1, 10, 2)
    x = x.requires_grad_()

    ln = nn.LayerNorm(normalized_shape=(x.shape[-1]), eps=1e-5)
    ln.weight = nn.Parameter(torch.tensor([0.5, 0.3]))
    ln.bias = nn.Parameter(torch.tensor([0.02, -0.01]))
    x_normalized = ln(x)

    print(x_normalized)

    weight = torch.tensor([0.12, -1.2, -0.7, 1.0, -1.3, 1.45,
                           0.08, 0.36, 1.6, -0.34, 0.87, 0.25], requires_grad=True)
    weight = nn.Parameter(weight.view(2, 6))
    bias = torch.tensor([0.02, 0.03, 0.05, -0.02, -0.03, -0.01], requires_grad=True)
    bias = nn.Parameter(bias.view(6))

    qkv_proj = x_normalized@weight + bias
    print(qkv_proj)

    query, key, value = torch.chunk(qkv_proj, 3, dim=-1)
    query = query.view(1, 10, 2, 1).transpose(1, 2)
    key = key.view(1, 10, 2, 1).transpose(1, 2)
    value = value.view(1, 10, 2, 1).transpose(1, 2)

    preattn = torch.matmul(query, key.transpose(-1, -2))
    preattn /= math.sqrt(1)
    print(preattn)

    attn = nn.functional.softmax(preattn, dim=-1)
    print(attn)

    y = torch.matmul(attn, value).transpose(1, 2).contiguous().view(1, 10, -1)
    print(y)
    */
   
    float x_normalized_truth[20] = {-0.479992, 0.289995, -0.479999, 0.290000, 0.519988, -0.309993, -0.479998, 0.289999, 0.519556, -0.309734,
                                    -0.479999, 0.290000, -0.480000, 0.290000, 0.519997, -0.309998, 0.519992, -0.309995, 0.519969, -0.309981};
    

    float qkv_proj_truth[60] = {-0.014399, 0.710388, 0.849986, -0.598590, 0.846285, -0.633489,
                                -0.014400, 0.710399, 0.849999, -0.598599, 0.846298, -0.633499,
                                0.057599, -0.705583, -0.809979, 0.605385, -0.975677, 0.666484,
                                -0.014400, 0.710397, 0.849996, -0.598597, 0.846296, -0.633497,
                                0.057568, -0.704971, -0.809263, 0.604865, -0.974891, 0.665923,
                                -0.014400, 0.710399, 0.849999, -0.598599, 0.846299, -0.633499,
                                -0.014400, 0.710400, 0.850000, -0.598600, 0.846300, -0.633500,
                                0.057600, -0.705596, -0.809995, 0.605397, -0.975695, 0.666496,
                                0.057599, -0.705588, -0.809986, 0.605390, -0.975685, 0.666489,
                                0.057598, -0.705557, -0.809949, 0.605363, -0.975644, 0.666460};

    float preattn_truth[200] = {-0.012239, -0.012239,  0.011663, -0.012239,  0.011653, -0.012239, -0.012239,  0.011663,  0.011663,  0.011663,
                                -0.012240, -0.012240,  0.011664, -0.012240,  0.011653, -0.012240, -0.012240,  0.011664,  0.011664,  0.011663,
                                 0.048958,  0.048959, -0.046654,  0.048959, -0.046613,  0.048959,  0.048959, -0.046655, -0.046654, -0.046652,
                                -0.012240, -0.012240,  0.011664, -0.012240,  0.011653, -0.012240, -0.012240,  0.011664,  0.011664,  0.011663,
                                 0.048932,  0.048933, -0.046629,  0.048933, -0.046588,  0.048933,  0.048933, -0.046630, -0.046629, -0.046627,
                                -0.012240, -0.012240,  0.011664, -0.012240,  0.011653, -0.012240, -0.012240,  0.011664,  0.011664,  0.011663,
                                -0.012240, -0.012240,  0.011664, -0.012240,  0.011653, -0.012240, -0.012240,  0.011664,  0.011664,  0.011663,
                                 0.048959,  0.048960, -0.046655,  0.048960, -0.046613,  0.048960,  0.048960, -0.046656, -0.046655, -0.046653,
                                 0.048959,  0.048959, -0.046654,  0.048959, -0.046613,  0.048959,  0.048959, -0.046655, -0.046655, -0.046653,
                                 0.048957,  0.048958, -0.046653,  0.048958, -0.046612,  0.048958,  0.048958, -0.046654, -0.046653, -0.046651,
                                -0.425231, -0.425238,  0.430058, -0.425236,  0.429689, -0.425238, -0.425238,  0.430067,  0.430062,  0.430043,
                                -0.425238, -0.425244,  0.430065, -0.425243,  0.429696, -0.425244, -0.425245,  0.430073,  0.430068,  0.430049,
                                 0.422355,  0.422361, -0.427149,  0.422360, -0.426783,  0.422361,  0.422362, -0.427157, -0.427153, -0.427134,
                                -0.425236, -0.425243,  0.430064, -0.425241,  0.429694, -0.425243, -0.425243,  0.430072,  0.430067,  0.430048,
                                 0.421989,  0.421995, -0.426779,  0.421994, -0.426413,  0.421995,  0.421996, -0.426787, -0.426783, -0.426764,
                                -0.425238, -0.425244,  0.430065, -0.425243,  0.429696, -0.425245, -0.425245,  0.430073,  0.430069,  0.430049,
                                -0.425238, -0.425245,  0.430065, -0.425243,  0.429696, -0.425245, -0.425245,  0.430074,  0.430069,  0.430050,
                                 0.422363,  0.422369, -0.427157,  0.422368, -0.426791,  0.422369,  0.422370, -0.427165, -0.427161, -0.427142,
                                 0.422358,  0.422365, -0.427153,  0.422363, -0.426786,  0.422365,  0.422365, -0.427161, -0.427156, -0.427137,
                                 0.422339,  0.422345, -0.427133,  0.422344, -0.426767,  0.422346,  0.422346, -0.427142, -0.427137, -0.427118};

    float attn_truth[200] = {0.098805, 0.098805, 0.101195, 0.098805, 0.101194, 0.098805, 0.098805, 0.101195, 0.101195, 0.101195,
                             0.098805, 0.098805, 0.101195, 0.098805, 0.101194, 0.098805, 0.098805, 0.101195, 0.101195, 0.101195,
                             0.104777, 0.104777, 0.095223, 0.104777, 0.095227, 0.104777, 0.104777, 0.095223, 0.095223, 0.095223,
                             0.098805, 0.098805, 0.101195, 0.098805, 0.101194, 0.098805, 0.098805, 0.101195, 0.101195, 0.101195,
                             0.104774, 0.104774, 0.095225, 0.104774, 0.095229, 0.104774, 0.104774, 0.095225, 0.095225, 0.095225,
                             0.098805, 0.098805, 0.101195, 0.098805, 0.101194, 0.098805, 0.098805, 0.101195, 0.101195, 0.101195,
                             0.098805, 0.098805, 0.101195, 0.098805, 0.101194, 0.098805, 0.098805, 0.101195, 0.101195, 0.101195,
                             0.104777, 0.104777, 0.095223, 0.104777, 0.095226, 0.104777, 0.104777, 0.095222, 0.095223, 0.095223,
                             0.104777, 0.104777, 0.095223, 0.104777, 0.095227, 0.104777, 0.104777, 0.095222, 0.095223, 0.095223,
                             0.104776, 0.104777, 0.095223, 0.104776, 0.095227, 0.104777, 0.104777, 0.095223, 0.095223, 0.095223,
                             0.059668, 0.059668, 0.140343, 0.059668, 0.140291, 0.059668, 0.059668, 0.140344, 0.140343, 0.140340,
                             0.059668, 0.059667, 0.140343, 0.059667, 0.140291, 0.059667, 0.059667, 0.140344, 0.140344, 0.140341,
                             0.140089, 0.140090, 0.059906, 0.140090, 0.059928, 0.140090, 0.140090, 0.059905, 0.059906, 0.059907,
                             0.059668, 0.059667, 0.140343, 0.059667, 0.140291, 0.059667, 0.059667, 0.140344, 0.140344, 0.140341,
                             0.140058, 0.140059, 0.059937, 0.140059, 0.059959, 0.140059, 0.140059, 0.059936, 0.059937, 0.059938,
                             0.059668, 0.059667, 0.140343, 0.059667, 0.140291, 0.059667, 0.059667, 0.140344, 0.140344, 0.140341,
                             0.059668, 0.059667, 0.140343, 0.059667, 0.140291, 0.059667, 0.059667, 0.140344, 0.140344, 0.140341,
                             0.140090, 0.140091, 0.059905, 0.140090, 0.059927, 0.140091, 0.140091, 0.059905, 0.059905, 0.059906,
                             0.140089, 0.140090, 0.059906, 0.140090, 0.059928, 0.140090, 0.140090, 0.059905, 0.059905, 0.059906,
                             0.140088, 0.140089, 0.059907, 0.140088, 0.059929, 0.140089, 0.140089, 0.059907, 0.059907, 0.059908};

    float y_truth[20] = {-0.075496,  0.278569, -0.075497,  0.278573, -0.021101, -0.244120, -0.075497,  0.278572, -0.021124, -0.243919,
                         -0.075497,  0.278573, -0.075497,  0.278573, -0.021100, -0.244124, -0.021101, -0.244122, -0.021102, -0.244111};

    float tolerance = 5e-6;

    layernorm_forward(x, mean, rstd, weight, bias, x_normalized, 1, 10, 2);
    matmul_forward(x_normalized, qkv_proj, w, b, 1, 10, 2, 6);
    attention_forward(qkv_proj, preattn, attn, y, 1, 10, 2, 2);

    for(int i=0; i<20; ++i){
        EXPECT_NEAR(x_normalized[i], x_normalized_truth[i], tolerance);
    }
    for(int i=0; i<60; ++i){
        EXPECT_NEAR(qkv_proj[i], qkv_proj_truth[i], tolerance);
    }
    for(int i=0; i<200; ++i){
        EXPECT_NEAR(preattn[i], preattn_truth[i], tolerance);
    }
    for(int i=0; i<200; ++i){
        EXPECT_NEAR(attn[i], attn_truth[i], tolerance);
    }
    for(int i=0; i<20; ++i){
        std::cout<<y[i]<<", "<<y_truth[i]<<std::endl;
        EXPECT_NEAR(y[i], y_truth[i], tolerance);
    }
}

int main(int argc, char **argv) {
    ::testing::InitGoogleTest(&argc, argv);
    return RUN_ALL_TESTS();
}